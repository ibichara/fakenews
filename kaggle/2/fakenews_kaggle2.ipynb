{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo vetor de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLASSPATH'] = 'stanford-pos'\n",
    "os.environ['STANFORD_MODELS'] = 'stanford-pos/models'\n",
    "st = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def postag_sentence(sentence_tokens):\n",
    "    return st.tag(sentence_tokens)\n",
    "\n",
    "def postag_filter(sentence_tokens):\n",
    "    pos_keep = ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', \n",
    "                'VBZ', 'NNP', 'NNPS','RB', 'RBR', 'RBS', 'JJ', 'JJR', 'JJS']\n",
    "    sentence_tokens_filtered = [pt[0] for pt in postag_sentence(sentence_tokens) if pt[1] in pos_keep]\n",
    "    return sentence_tokens_filtered\n",
    "\n",
    "def stem_token(token):\n",
    "    return porter_stemmer.stem(token)\n",
    "\n",
    "def tokenize_doc(doc, use_pt_filter=False,\n",
    "                      use_lowercase_filter=False,\n",
    "                      use_stopwords_filter=False,\n",
    "                      use_stemming_filter=False):\n",
    "    sentences = sent_tokenize(doc)\n",
    "    tokens = []\n",
    "    for s in sentences:\n",
    "        tk_sentence = word_tokenize(s)\n",
    "        if use_pt_filter:\n",
    "            tk_sentence = postag_filter(tk_sentence)\n",
    "        if use_lowercase_filter:\n",
    "            tk_sentence = [tk.lower() for tk in tk_sentence]\n",
    "        if use_stopwords_filter:\n",
    "            tk_sentence = [tk for tk in tk_sentence if tk not in stopwords.words('english')]\n",
    "        if use_stemming_filter:\n",
    "            tk_sentence = [stem_token(tk) for tk in tk_sentence ]\n",
    "        tokens.extend(tk_sentence)\n",
    "        \n",
    "    return tokens\n",
    "        \n",
    "def remove_punctuation(tokens):\n",
    "    return [ t for t in tokens if t not in string.punctuation ]\n",
    "\n",
    "def get_vocabulary_tokenized_from_docs(tk_docs):\n",
    "    vocabulary = Counter()\n",
    "    for tk_d in tk_docs:\n",
    "        for tk in tk_d:\n",
    "            vocabulary[tk] += 1\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for index, row in train_df.iterrows():\n",
    "    docs.append(' '.join([str(row['title']), str(row['text'])]).replace(\"’\", \"'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulario e reducao de dimensionalidade por vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem Pos-Tagging, Sem Lowercase, Sem remoção Stopwords, Sem Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_docs = [tokenize_doc(d, False, False, False, False) for d in docs]\n",
    "vocabulary = get_vocabulary_tokenized_from_docs(tk_docs)\n",
    "print('Tamanho do vocabulario inicial:', len(vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com Pos-Tagging, Sem Lowercase, Sem remoção Stopwords, Sem Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_docs = [tokenize_doc(d, True, False, False, False) for d in docs]\n",
    "vocabulary = get_vocabulary_tokenized_from_docs(tk_docs)\n",
    "print('Tamanho do vocabulario inicial:', len(vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com Pos-Tagging, Com Lowercase, Sem remoção Stopwords, Sem Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_docs = [tokenize_doc(d, True, True, False, False) for d in docs]\n",
    "vocabulary = get_vocabulary_tokenized_from_docs(tk_docs)\n",
    "print('Tamanho do vocabulario com lowercase:', len(vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com Pos-Tagging, Com Lowercase, Com remoção Stopwords, Sem Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_docs = [tokenize_doc(d, True, True, True, False) for d in docs]\n",
    "vocabulary = get_vocabulary_tokenized_from_docs(tk_docs)\n",
    "print('Tamanho do vocabulario com lowercase:', len(vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com Pos-Tagging, Com Lowercase, Com remoção Stopwords, Com Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_docs = [tokenize_doc(d, True, True, True, True) for d in docs]\n",
    "vocabulary = get_vocabulary_tokenized_from_docs(tk_docs)\n",
    "print('Tamanho do vocabulario com lowercase:', len(vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adicionar ngramas?, verificar dimensao, criar matriz de features, rodar naive bayes para baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fakenews]",
   "language": "python",
   "name": "conda-env-fakenews-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
